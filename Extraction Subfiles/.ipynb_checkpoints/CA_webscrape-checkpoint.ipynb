{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to remove duplicates (by job_id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pymongo\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "country = \"ca\"\n",
    "\n",
    "job_title_list = []\n",
    "job_title_index = []\n",
    "company_list = []\n",
    "job_id_list = []\n",
    "location_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analyst (index = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ca.indeed.com/jobs?q=Data+Analyst&radius=25&start=0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# search query for Data Analyst roles\n",
    "url = f\"https://ca.indeed.com/jobs?q=Data+Analyst&radius=25&start=0\"\n",
    "print(url)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "results = soup.find('div', id='searchCountPages')\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually checked how many pages there are (750 pages)\n",
    "page = range(0,760,10)\n",
    "\n",
    "page_string = map(str, page) \n",
    "\n",
    "for page in list(page_string): \n",
    "    url = f\"https://{country}.indeed.com/jobs?q=Data+Analyst&radius=25&start={page}\"\n",
    "    print(url)\n",
    "    \n",
    "    # Retrieve page with the requests module\n",
    "    response = requests.get(url)\n",
    "    # Create BeautifulSoup object; parse with 'html.parser'\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    # Retrieve the parent divs for all articles\n",
    "    results = soup.find_all('div', class_='result')\n",
    "    \n",
    "    # loop over results to get article data\n",
    "    for result in results:\n",
    "        try:\n",
    "            # scrape the article header \n",
    "            job_title = result.find('a', class_='jobtitle').text.strip()\n",
    "            job_index = 1\n",
    "            company = result.find('span', class_='company').text.strip()\n",
    "            job_id = result.get('id')\n",
    "            location = result.find(class_='location').text\n",
    "\n",
    "            job_title_list.append(job_title)\n",
    "            job_title_index.append(job_index)\n",
    "            company_list.append(company)\n",
    "            location_list.append(location)\n",
    "            job_id_list.append(job_id)\n",
    "            \n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = set(job_id_list)\n",
    "len(unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scientist (index = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search query for Data Scientist roles\n",
    "url = f\"https://ca.indeed.com/jobs?q=Data+Scientist&radius=25&start=0\"\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "results = soup.find(id='searchCountPages').text.strip()\n",
    "\n",
    "print(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually checked how many pages there are (350 pages)\n",
    "page = range(0,360,10)\n",
    "\n",
    "page_string = map(str, page) \n",
    "\n",
    "for page in list(page_string): \n",
    "    url = f\"https://{country}.indeed.com/jobs?q=Data+Scientist&radius=25&start={page}\"\n",
    "    print(url)\n",
    "    \n",
    "    # Retrieve page with the requests module\n",
    "    response = requests.get(url)\n",
    "    # Create BeautifulSoup object; parse with 'html.parser'\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    # Retrieve the parent divs for all articles\n",
    "    results = soup.find_all('div', class_='result')\n",
    "    \n",
    "    # loop over results to get article data\n",
    "    for result in results:\n",
    "        try:\n",
    "            # scrape the article header \n",
    "            job_title = result.find('a', class_='jobtitle').text.strip()\n",
    "            job_index = 2\n",
    "            company = result.find('span', class_='company').text.strip()\n",
    "            job_id = result.get('id')\n",
    "            location = result.find(class_='location').text\n",
    "\n",
    "            job_title_list.append(job_title)\n",
    "            job_title_index.append(job_index)\n",
    "            company_list.append(company)\n",
    "            location_list.append(location)\n",
    "            job_id_list.append(job_id)\n",
    "            \n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = set(job_id_list)\n",
    "len(unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineer (index = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search query for Data Engineer roles\n",
    "url = f\"https://ca.indeed.com/jobs?q=Data+Engineer&l=&radius=25\"\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "results = soup.find(id='searchCountPages').text.strip()\n",
    "\n",
    "print(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only one page\n",
    "\n",
    "url = f\"https://{country}.indeed.com/jobs?q=Data+Engineer&l=&radius=25\"\n",
    "print(url)\n",
    "    \n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url)\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "# Retrieve the parent divs for all articles\n",
    "results = soup.find_all('div', class_='result')\n",
    "    \n",
    "# loop over results to get article data\n",
    "for result in results:\n",
    "    try:\n",
    "        # scrape the article header \n",
    "        job_title = result.find('a', class_='jobtitle').text.strip()\n",
    "        job_index = 3\n",
    "        company = result.find('span', class_='company').text.strip()\n",
    "        job_id = result.get('id')\n",
    "        location = result.find(class_='location').text\n",
    "\n",
    "        job_title_list.append(job_title)\n",
    "        job_title_index.append(job_index)\n",
    "        company_list.append(company)\n",
    "        location_list.append(location)\n",
    "        job_id_list.append(job_id)\n",
    "            \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = set(job_id_list)\n",
    "len(unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning (index = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search query for Machine Learning roles\n",
    "url = f\"https://ca.indeed.com/jobs?q=Machine+Learning&l=&radius=25&start=0\"\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "results = soup.find(id='searchCountPages').text.strip()\n",
    "\n",
    "print(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually checked how many pages there are (740 pages)\n",
    "page = range(0,750,10)\n",
    "\n",
    "page_string = map(str, page) \n",
    "\n",
    "for page in list(page_string): \n",
    "    url = f\"https://{country}.indeed.com/jobs?q=Machine+Learning&l=&radius=25&start={page}\"\n",
    "    print(url)\n",
    "    \n",
    "    # Retrieve page with the requests module\n",
    "    response = requests.get(url)\n",
    "    # Create BeautifulSoup object; parse with 'html.parser'\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    # Retrieve the parent divs for all articles\n",
    "    results = soup.find_all('div', class_='result')\n",
    "    \n",
    "    # loop over results to get article data\n",
    "    for result in results:\n",
    "        try:\n",
    "            # scrape the article header \n",
    "            job_title = result.find('a', class_='jobtitle').text.strip()\n",
    "            job_index = 4\n",
    "            company = result.find('span', class_='company').text.strip()\n",
    "            job_id = result.get('id')\n",
    "            location = result.find(class_='location').text\n",
    "\n",
    "            job_title_list.append(job_title)\n",
    "            job_title_index.append(job_index)\n",
    "            company_list.append(company)\n",
    "            location_list.append(location)\n",
    "            job_id_list.append(job_id)\n",
    "            \n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = set(job_id_list)\n",
    "len(unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Compilation into one table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting list into dataframe\n",
    "\n",
    "CA_jobmarket = {\"Job ID\" : job_id_list,\n",
    "                \"Job Title Index\" : job_title_index,\n",
    "                \"Job Title\" : job_title_list, \n",
    "                \"Company Name\" : company_list, \n",
    "                \"Company Location\" : location_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CA_jobmarket_df = pd.DataFrame(CA_jobmarket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CA_jobmarket_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CA_jobmarket_df.to_csv(\"../Clean Data/CA-JobMarket.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
