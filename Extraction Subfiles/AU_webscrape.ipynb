{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to remove duplicates (by job_id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "country = \"au\"\n",
    "\n",
    "job_title_list = []\n",
    "job_title_index = []\n",
    "company_list = []\n",
    "job_id_list = []\n",
    "location_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analyst (index = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=0\n",
      "<div id=\"searchCountPages\">\n",
      "                    Page 1 of 1,484 jobs</div>\n"
     ]
    }
   ],
   "source": [
    "# search query for Data Analyst roles\n",
    "url = f\"https://{country}.indeed.com/jobs?q=Data+Analyst&radius=25&start=0\"\n",
    "print(url)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "results = soup.find('div', id='searchCountPages')\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=0\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=10\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=20\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=30\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=40\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=50\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=60\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=70\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=80\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=90\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=100\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=110\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=120\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=130\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=140\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=150\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=160\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=170\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=180\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=190\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=200\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=210\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=220\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=230\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=240\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=250\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=260\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=270\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=280\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=290\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=300\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=310\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=320\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=330\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=340\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=350\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=360\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=370\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=380\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=390\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=400\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=410\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=420\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=430\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=440\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=450\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=460\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=470\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=480\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=490\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=500\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=510\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=520\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=530\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=540\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=550\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=560\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=570\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=580\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=590\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=600\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=610\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=620\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=630\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=640\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=650\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=660\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=670\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=680\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=690\n",
      "https://au.indeed.com/jobs?q=Data+Analyst&radius=25&start=700\n"
     ]
    }
   ],
   "source": [
    "# manually checked how many pages there are (750 pages)\n",
    "page = range(0,710,10)\n",
    "\n",
    "page_string = map(str, page) \n",
    "\n",
    "for page in list(page_string): \n",
    "    url = f\"https://{country}.indeed.com/jobs?q=Data+Analyst&radius=25&start={page}\"\n",
    "    print(url)\n",
    "    time_gap = random.randrange(3, 10, 1)\n",
    "    time.sleep(time_gap)\n",
    "    \n",
    "    # Retrieve page with the requests module\n",
    "    response = requests.get(url)\n",
    "    # Create BeautifulSoup object; parse with 'html.parser'\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    # Retrieve the parent divs for all articles\n",
    "    results = soup.find_all('div', class_='result')\n",
    "    \n",
    "    # loop over results to get article data\n",
    "    for result in results:\n",
    "        try:\n",
    "            # scrape the article header \n",
    "            job_title = result.find('a', class_='jobtitle').text.strip()\n",
    "            job_index = 1\n",
    "            company = result.find('span', class_='company').text.strip()\n",
    "            job_id = result.get('id')\n",
    "            location = result.find(class_='location').text\n",
    "\n",
    "            job_title_list.append(job_title)\n",
    "            job_title_index.append(job_index)\n",
    "            company_list.append(company)\n",
    "            location_list.append(location)\n",
    "            job_id_list.append(job_id)\n",
    "            \n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "767"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique = set(job_id_list)\n",
    "len(unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scientist (index = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 of 230 jobs\n"
     ]
    }
   ],
   "source": [
    "# search query for Data Scientist roles\n",
    "url = f\"https://{country}.indeed.com/jobs?q=Data+Scientist&radius=25&start=0\"\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "results = soup.find(id='searchCountPages').text.strip()\n",
    "\n",
    "print(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=0\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=10\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=20\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=30\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=40\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=50\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=60\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=70\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=80\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=90\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=100\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=110\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=120\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=130\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=140\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=150\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=160\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=170\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=180\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=190\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=200\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=210\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=220\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=230\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=240\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=250\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=260\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=270\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=280\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=290\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=300\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=310\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=320\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=330\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=340\n",
      "https://au.indeed.com/jobs?q=Data+Scientist&radius=25&start=350\n"
     ]
    }
   ],
   "source": [
    "# manually checked how many pages there are (350 pages)\n",
    "page = range(0,360,10)\n",
    "\n",
    "\n",
    "page_string = map(str, page) \n",
    "\n",
    "for page in list(page_string): \n",
    "    url = f\"https://{country}.indeed.com/jobs?q=Data+Scientist&radius=25&start={page}\"\n",
    "    print(url)\n",
    "    time_gap = random.randrange(3, 10, 1)\n",
    "    time.sleep(time_gap)\n",
    "    \n",
    "    # Retrieve page with the requests module\n",
    "    response = requests.get(url)\n",
    "    # Create BeautifulSoup object; parse with 'html.parser'\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    # Retrieve the parent divs for all articles\n",
    "    results = soup.find_all('div', class_='result')\n",
    "    \n",
    "    # loop over results to get article data\n",
    "    for result in results:\n",
    "        try:\n",
    "            # scrape the article header \n",
    "            job_title = result.find('a', class_='jobtitle').text.strip()\n",
    "            job_index = 2\n",
    "            company = result.find('span', class_='company').text.strip()\n",
    "            job_id = result.get('id')\n",
    "            location = result.find(class_='location').text\n",
    "\n",
    "            job_title_list.append(job_title)\n",
    "            job_title_index.append(job_index)\n",
    "            company_list.append(company)\n",
    "            location_list.append(location)\n",
    "            job_id_list.append(job_id)\n",
    "            \n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique = set(job_id_list)\n",
    "len(unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineer (index = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 of 2,530 jobs\n"
     ]
    }
   ],
   "source": [
    "# search query for Data Engineer roles\n",
    "url = f\"https://{country}.indeed.com/jobs?q=Data+Engineer&l=&radius=25\"\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "results = soup.find(id='searchCountPages').text.strip()\n",
    "\n",
    "print(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://au.indeed.com/jobs?q=Data+Engineer&l=&radius=25\n"
     ]
    }
   ],
   "source": [
    "# only one page\n",
    "\n",
    "url = f\"https://{country}.indeed.com/jobs?q=Data+Engineer&l=&radius=25\"\n",
    "print(url)\n",
    "    \n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url)\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "# Retrieve the parent divs for all articles\n",
    "results = soup.find_all('div', class_='result')\n",
    "    \n",
    "# loop over results to get article data\n",
    "for result in results:\n",
    "    try:\n",
    "        # scrape the article header \n",
    "        job_title = result.find('a', class_='jobtitle').text.strip()\n",
    "        job_index = 3\n",
    "        company = result.find('span', class_='company').text.strip()\n",
    "        job_id = result.get('id')\n",
    "        location = result.find(class_='location').text\n",
    "\n",
    "        job_title_list.append(job_title)\n",
    "        job_title_index.append(job_index)\n",
    "        company_list.append(company)\n",
    "        location_list.append(location)\n",
    "        job_id_list.append(job_id)\n",
    "            \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "952"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique = set(job_id_list)\n",
    "len(unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning (index = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-a2f3a978197c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Create BeautifulSoup object; parse with 'html.parser'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lxml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'searchCountPages'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "# search query for Machine Learning roles\n",
    "url = f\"https://{country}indeed.com/jobs?q=Machine+Learning&l=&radius=25&start=0\"\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "results = soup.find(id='searchCountPages').text.strip()\n",
    "\n",
    "print(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually checked how many pages there are (740 pages)\n",
    "page = range(0,710,10)\n",
    "\n",
    "\n",
    "page_string = map(str, page) \n",
    "\n",
    "for page in list(page_string): \n",
    "    url = f\"https://{country}.indeed.com/jobs?q=Machine+Learning&l=&radius=25&start={page}\"\n",
    "    print(url)\n",
    "    time_gap = random.randrange(3, 10, 1)\n",
    "    time.sleep(time_gap)\n",
    "    \n",
    "    # Retrieve page with the requests module\n",
    "    response = requests.get(url)\n",
    "    # Create BeautifulSoup object; parse with 'html.parser'\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    # Retrieve the parent divs for all articles\n",
    "    results = soup.find_all('div', class_='result')\n",
    "    \n",
    "    # loop over results to get article data\n",
    "    for result in results:\n",
    "        try:\n",
    "            # scrape the article header \n",
    "            job_title = result.find('a', class_='jobtitle').text.strip()\n",
    "            job_index = 4\n",
    "            company = result.find('span', class_='company').text.strip()\n",
    "            job_id = result.get('id')\n",
    "            location = result.find(class_='location').text\n",
    "\n",
    "            job_title_list.append(job_title)\n",
    "            job_title_index.append(job_index)\n",
    "            company_list.append(company)\n",
    "            location_list.append(location)\n",
    "            job_id_list.append(job_id)\n",
    "            \n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = set(job_id_list)\n",
    "len(unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Compilation into one table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting list into dataframe\n",
    "\n",
    "AU_jobmarket = {\"Job ID\" : job_id_list,\n",
    "                \"Job Title Index\" : job_title_index,\n",
    "                \"Job Title\" : job_title_list, \n",
    "                \"Company Name\" : company_list, \n",
    "                \"Company Location\" : location_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "AU_jobmarket_df = pd.DataFrame(AU_jobmarket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job ID</th>\n",
       "      <th>Job Title Index</th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Company Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p_447bbc295950e8b9</td>\n",
       "      <td>1</td>\n",
       "      <td>Data Reporting Analyst</td>\n",
       "      <td>Best Friends</td>\n",
       "      <td>Altona North VIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p_335dc79e724048f5</td>\n",
       "      <td>1</td>\n",
       "      <td>Data Insights Analyst P&amp;C</td>\n",
       "      <td>Coles Careers</td>\n",
       "      <td>Hawthorn East VIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p_6cfb7acbba754c53</td>\n",
       "      <td>1</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>PlaySide Studios</td>\n",
       "      <td>Melbourne VIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p_fd7300023b1af3f8</td>\n",
       "      <td>1</td>\n",
       "      <td>Data Analyst - PACE Tribe</td>\n",
       "      <td>ANZ Banking Group</td>\n",
       "      <td>Melbourne VIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p_8b5985f9e36e8a21</td>\n",
       "      <td>1</td>\n",
       "      <td>Senior Policy and Data Analyst (VPSG5)</td>\n",
       "      <td>Department of Education and Training</td>\n",
       "      <td>Melbourne VIC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Job ID  Job Title Index  \\\n",
       "0  p_447bbc295950e8b9                1   \n",
       "1  p_335dc79e724048f5                1   \n",
       "2  p_6cfb7acbba754c53                1   \n",
       "3  p_fd7300023b1af3f8                1   \n",
       "4  p_8b5985f9e36e8a21                1   \n",
       "\n",
       "                                Job Title  \\\n",
       "0                  Data Reporting Analyst   \n",
       "1               Data Insights Analyst P&C   \n",
       "2                            Data Analyst   \n",
       "3               Data Analyst - PACE Tribe   \n",
       "4  Senior Policy and Data Analyst (VPSG5)   \n",
       "\n",
       "                           Company Name   Company Location  \n",
       "0                          Best Friends   Altona North VIC  \n",
       "1                         Coles Careers  Hawthorn East VIC  \n",
       "2                      PlaySide Studios      Melbourne VIC  \n",
       "3                     ANZ Banking Group      Melbourne VIC  \n",
       "4  Department of Education and Training      Melbourne VIC  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AU_jobmarket_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "AU_jobmarket_df.to_csv(\"../Clean Data/AU-JobMarket.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
