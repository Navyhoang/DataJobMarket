{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscrape for Data Related Jobs in the United States\n",
    "----\n",
    "4 queries were made into Indeed.com to obtain job postings for Data Analyst, Data Engineer, Data Scientist, and Machine Learning. Due to large quantities of job postings, a limit of 70 pages were extracted (equaling around 1000 job posts) for each role. \n",
    "\n",
    "Job title index was also assigned to each job posting during the web scrape. For example, while scraping for Data Analyst roles, an index number of 1 was assigned to each posting. This will help for analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list creation to store data from webscrape\n",
    "\n",
    "job_title_list = []\n",
    "job_title_index = []\n",
    "company_list = []\n",
    "job_id_list = []\n",
    "location_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analyst (index = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.indeed.com/jobs?q=data+analyst&l=\n",
      "Page 1 of 16,728 jobs\n"
     ]
    }
   ],
   "source": [
    "# search query for Data Analyst roles\n",
    "url = f\"https://www.indeed.com/jobs?q=data+analyst&l=\"\n",
    "print(url)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "# Collecting data on total number of job postings \n",
    "results = soup.find('div', id='searchCountPages').text.strip()\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.indeed.com/jobs?q=data+analyst&start=0\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=10\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=20\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=30\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=40\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=50\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=60\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=70\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=80\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=90\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=100\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=110\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=120\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=130\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=140\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=150\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=160\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=170\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=180\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=190\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=200\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=210\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=220\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=230\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=240\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=250\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=260\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=270\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=280\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=290\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=300\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=310\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=320\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=330\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=340\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=350\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=360\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=370\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=380\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=390\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=400\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=410\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=420\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=430\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=440\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=450\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=460\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=470\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=480\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=490\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=500\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=510\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=520\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=530\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=540\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=550\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=560\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=570\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=580\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=590\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=600\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=610\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=620\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=630\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=640\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=650\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=660\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=670\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=680\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=690\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=700\n"
     ]
    }
   ],
   "source": [
    "# Scraping to page limit of 70 pages\n",
    "page = range(0,710,10)\n",
    "\n",
    "page_string = map(str, page) \n",
    "\n",
    "for page in list(page_string): \n",
    "    url = f\"https://www.indeed.com/jobs?q=data+analyst&start={page}\"\n",
    "    print(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Retrieve page with the requests module\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Create BeautifulSoup object; parse with 'lxml'\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    # Retrieve the parent divs for all job postings\n",
    "    results = soup.find_all('div', class_='result')\n",
    "    \n",
    "    # loop over results to get job post data\n",
    "    for result in results:\n",
    "        try:\n",
    "            job_title = result.find('a', class_='jobtitle').text.strip()\n",
    "            job_index = 1\n",
    "            company = result.find('span', class_='company').text.strip()\n",
    "            job_id = result.get('id')\n",
    "            location = result.find(class_='location').text\n",
    "            \n",
    "            # adding data into lists \n",
    "            job_title_list.append(job_title)\n",
    "            job_title_index.append(job_index)\n",
    "            company_list.append(company)\n",
    "            location_list.append(location)\n",
    "            job_id_list.append(job_id)\n",
    "            \n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "681"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of unique jobs added into dataset \n",
    "unique = set(job_id_list)\n",
    "len(unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scientist (index = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search query for Data Scientist roles\n",
    "url = f\"https://www.indeed.com/jobs?q=data+scientist&l=\"\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "# Collecting data on total number of job postings \n",
    "results = soup.find(id='searchCountPages').text.strip()\n",
    "\n",
    "print(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping to page limit of 70 pages\n",
    "page = range(0,710,10)\n",
    "\n",
    "page_string = map(str, page) \n",
    "\n",
    "for page in list(page_string): \n",
    "    url = f\"https://www.indeed.com/jobs?q=data+scientist&start={page}\"\n",
    "    print(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Retrieve page with the requests module\n",
    "    response = requests.get(url)\n",
    "    # Create BeautifulSoup object; parse with 'lxml'\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    # Retrieve the parent divs for all postings\n",
    "    results = soup.find_all('div', class_='result')\n",
    "    \n",
    "    # Loop over results to get job posting data\n",
    "    for result in results:\n",
    "        try:\n",
    "            job_title = result.find('a', class_='jobtitle').text.strip()\n",
    "            job_index = 2\n",
    "            company = result.find('span', class_='company').text.strip()\n",
    "            job_id = result.get('id')\n",
    "            location = result.find(class_='location').text\n",
    "\n",
    "            # Adding data to lists\n",
    "            job_title_list.append(job_title)\n",
    "            job_title_index.append(job_index)\n",
    "            company_list.append(company)\n",
    "            location_list.append(location)\n",
    "            job_id_list.append(job_id)\n",
    "            \n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique jobs added into dataset \n",
    "unique = set(job_id_list)\n",
    "len(unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineer (index = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search query for Data Engineer roles\n",
    "url = f\"https://www.indeed.com/jobs?q=data+engineer&l=\"\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "results = soup.find(id='searchCountPages').text.strip()\n",
    "\n",
    "print(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping to page limit of 70 pages\n",
    "page = range(0,710,10)\n",
    "\n",
    "# Changing integer to string\n",
    "page_string = map(str, page) \n",
    "\n",
    "for page in list(page_string): \n",
    "    url = f\"https://www.indeed.com/jobs?q=data+engineer&start={page}\"\n",
    "    print(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Retrieve page with the requests module\n",
    "    response = requests.get(url)\n",
    "    # Create BeautifulSoup object; parse with 'lxml'\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    # Retrieve the parent divs for all postings\n",
    "    results = soup.find_all('div', class_='result')\n",
    "    \n",
    "    # loop over results to get job posting data\n",
    "    for result in results:\n",
    "        try:\n",
    "            job_title = result.find('a', class_='jobtitle').text.strip()\n",
    "            job_index = 3\n",
    "            company = result.find('span', class_='company').text.strip()\n",
    "            job_id = result.get('id')\n",
    "            location = result.find(class_='location').text\n",
    "\n",
    "            # Adding data to lists\n",
    "            job_title_list.append(job_title)\n",
    "            job_title_index.append(job_index)\n",
    "            company_list.append(company)\n",
    "            location_list.append(location)\n",
    "            job_id_list.append(job_id)\n",
    "            \n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique jobs added into dataset \n",
    "unique = set(job_id_list)\n",
    "len(unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning (index = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search query for Machine Learning roles\n",
    "url = f\"https://www.indeed.com/jobs?q=machine+learning&l=\"\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "results = soup.find(id='searchCountPages').text.strip()\n",
    "\n",
    "print(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually checked how many pages there are\n",
    "page = range(0,710,10)\n",
    "\n",
    "# Changing integer to string\n",
    "page_string = map(str, page) \n",
    "\n",
    "for page in list(page_string): \n",
    "    url = f\"https://www.indeed.com/jobs?q=machine+learning&start={page}\"\n",
    "    print(url)\n",
    "    \n",
    "    # Retrieve page with the requests module\n",
    "    response = requests.get(url)\n",
    "    # Create BeautifulSoup object; parse with 'lxml'\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    # Retrieve the parent divs for all postings\n",
    "    results = soup.find_all('div', class_='result')\n",
    "    \n",
    "    # loop over results to get job posting data\n",
    "    for result in results:\n",
    "        try:\n",
    "            job_title = result.find('a', class_='jobtitle').text.strip()\n",
    "            job_index = 4\n",
    "            company = result.find('span', class_='company').text.strip()\n",
    "            job_id = result.get('id')\n",
    "            location = result.find(class_='location').text\n",
    "\n",
    "            # Adding data to lists\n",
    "            job_title_list.append(job_title)\n",
    "            job_title_index.append(job_index)\n",
    "            company_list.append(company)\n",
    "            location_list.append(location)\n",
    "            job_id_list.append(job_id)\n",
    "            \n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique jobs added into dataset \n",
    "unique = set(job_id_list)\n",
    "len(unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Compilation into one table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling all lists into one dataframe\n",
    "\n",
    "US_jobmarket = {\"Job ID\" : job_id_list,\n",
    "                \"Job Title Index\" : job_title_index,\n",
    "                \"Job Title\" : job_title_list, \n",
    "                \"Company Name\" : company_list, \n",
    "                \"Company Location\" : location_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_jobmarket_df = pd.DataFrame(US_jobmarket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_jobmarket_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv file \n",
    "US_jobmarket_df.to_csv(\"../Clean Data/US-JobMarket.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
