{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to remove duplicates (by job_id) \n",
    "max 500 pages (150 data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "\n",
    "job_title_list = []\n",
    "job_title_index = []\n",
    "company_list = []\n",
    "job_id_list = []\n",
    "location_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analyst (index = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.indeed.com/jobs?q=data+analyst&l=\n",
      "Page 1 of 16,728 jobs\n"
     ]
    }
   ],
   "source": [
    "# search query for Data Analyst roles\n",
    "url = f\"https://www.indeed.com/jobs?q=data+analyst&l=\"\n",
    "print(url)\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "results = soup.find('div', id='searchCountPages').text.strip()\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.indeed.com/jobs?q=data+analyst&start=0\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=10\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=20\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=30\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=40\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=50\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=60\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=70\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=80\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=90\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=100\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=110\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=120\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=130\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=140\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=150\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=160\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=170\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=180\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=190\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=200\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=210\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=220\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=230\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=240\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=250\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=260\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=270\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=280\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=290\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=300\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=310\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=320\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=330\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=340\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=350\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=360\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=370\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=380\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=390\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=400\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=410\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=420\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=430\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=440\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=450\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=460\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=470\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=480\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=490\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=500\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=510\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=520\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=530\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=540\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=550\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=560\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=570\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=580\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=590\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=600\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=610\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=620\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=630\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=640\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=650\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=660\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=670\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=680\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=690\n",
      "https://www.indeed.com/jobs?q=data+analyst&start=700\n"
     ]
    }
   ],
   "source": [
    "# manually checked how many pages there are (859 pages)\n",
    "page = range(0,710,10)\n",
    "\n",
    "page_string = map(str, page) \n",
    "\n",
    "for page in list(page_string): \n",
    "    url = f\"https://www.indeed.com/jobs?q=data+analyst&start={page}\"\n",
    "    print(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Retrieve page with the requests module\n",
    "    response = requests.get(url)\n",
    "    # Create BeautifulSoup object; parse with 'html.parser'\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    # Retrieve the parent divs for all articles\n",
    "    results = soup.find_all('div', class_='result')\n",
    "    \n",
    "    # loop over results to get article data\n",
    "    for result in results:\n",
    "        try:\n",
    "            # scrape the article header \n",
    "            job_title = result.find('a', class_='jobtitle').text.strip()\n",
    "            job_index = 1\n",
    "            company = result.find('span', class_='company').text.strip()\n",
    "            job_id = result.get('id')\n",
    "            location = result.find(class_='location').text\n",
    "\n",
    "            job_title_list.append(job_title)\n",
    "            job_title_index.append(job_index)\n",
    "            company_list.append(company)\n",
    "            location_list.append(location)\n",
    "            job_id_list.append(job_id)\n",
    "            \n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "681"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique = set(job_id_list)\n",
    "len(unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scientist (index = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-ff2fdbb14107>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Create BeautifulSoup object; parse with 'html.parser'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lxml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'searchCountPages'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "# search query for Data Scientist roles\n",
    "url = f\"https://www.indeed.com/jobs?q=data+scientist&l=\"\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "results = soup.find(id='searchCountPages').text.strip()\n",
    "\n",
    "print(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually checked how many pages there are (770 pages)\n",
    "page = range(0,710,10)\n",
    "\n",
    "page_string = map(str, page) \n",
    "\n",
    "for page in list(page_string): \n",
    "    url = f\"https://www.indeed.com/jobs?q=data+scientist&start={page}\"\n",
    "    print(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Retrieve page with the requests module\n",
    "    response = requests.get(url)\n",
    "    # Create BeautifulSoup object; parse with 'html.parser'\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    # Retrieve the parent divs for all articles\n",
    "    results = soup.find_all('div', class_='result')\n",
    "    \n",
    "    # loop over results to get article data\n",
    "    for result in results:\n",
    "        try:\n",
    "            # scrape the article header \n",
    "            job_title = result.find('a', class_='jobtitle').text.strip()\n",
    "            job_index = 2\n",
    "            company = result.find('span', class_='company').text.strip()\n",
    "            job_id = result.get('id')\n",
    "            location = result.find(class_='location').text\n",
    "\n",
    "            job_title_list.append(job_title)\n",
    "            job_title_index.append(job_index)\n",
    "            company_list.append(company)\n",
    "            location_list.append(location)\n",
    "            job_id_list.append(job_id)\n",
    "            \n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = set(job_id_list)\n",
    "len(unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineer (index = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search query for Data Engineer roles\n",
    "url = f\"https://www.indeed.com/jobs?q=data+engineer&l=\"\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "results = soup.find(id='searchCountPages').text.strip()\n",
    "\n",
    "print(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually checked how many pages there are (770 pages)\n",
    "page = range(0,710,10)\n",
    "\n",
    "page_string = map(str, page) \n",
    "\n",
    "for page in list(page_string): \n",
    "    url = f\"https://www.indeed.com/jobs?q=data+engineer&start={page}\"\n",
    "    print(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Retrieve page with the requests module\n",
    "    response = requests.get(url)\n",
    "    # Create BeautifulSoup object; parse with 'html.parser'\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    # Retrieve the parent divs for all articles\n",
    "    results = soup.find_all('div', class_='result')\n",
    "    \n",
    "    # loop over results to get article data\n",
    "    for result in results:\n",
    "        try:\n",
    "            # scrape the article header \n",
    "            job_title = result.find('a', class_='jobtitle').text.strip()\n",
    "            job_index = 3\n",
    "            company = result.find('span', class_='company').text.strip()\n",
    "            job_id = result.get('id')\n",
    "            location = result.find(class_='location').text\n",
    "\n",
    "            job_title_list.append(job_title)\n",
    "            job_title_index.append(job_index)\n",
    "            company_list.append(company)\n",
    "            location_list.append(location)\n",
    "            job_id_list.append(job_id)\n",
    "            \n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = set(job_id_list)\n",
    "len(unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning (index = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search query for Machine Learning roles\n",
    "url = f\"https://www.indeed.com/jobs?q=machine+learning&l=\"\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "results = soup.find(id='searchCountPages').text.strip()\n",
    "\n",
    "print(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually checked how many pages there are\n",
    "page = range(0,710,10)\n",
    "\n",
    "page_string = map(str, page) \n",
    "\n",
    "for page in list(page_string): \n",
    "    url = f\"https://www.indeed.com/jobs?q=machine+learning&start={page}\"\n",
    "    print(url)\n",
    "    \n",
    "    # Retrieve page with the requests module\n",
    "    response = requests.get(url)\n",
    "    # Create BeautifulSoup object; parse with 'html.parser'\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    # Retrieve the parent divs for all articles\n",
    "    results = soup.find_all('div', class_='result')\n",
    "    \n",
    "    # loop over results to get article data\n",
    "    for result in results:\n",
    "        try:\n",
    "            # scrape the article header \n",
    "            job_title = result.find('a', class_='jobtitle').text.strip()\n",
    "            job_index = 4\n",
    "            company = result.find('span', class_='company').text.strip()\n",
    "            job_id = result.get('id')\n",
    "            location = result.find(class_='location').text\n",
    "\n",
    "            job_title_list.append(job_title)\n",
    "            job_title_index.append(job_index)\n",
    "            company_list.append(company)\n",
    "            location_list.append(location)\n",
    "            job_id_list.append(job_id)\n",
    "            \n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = set(job_id_list)\n",
    "len(unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Compilation into one table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting list into dataframe\n",
    "\n",
    "US_jobmarket = {\"Job ID\" : job_id_list,\n",
    "                \"Job Title Index\" : job_title_index,\n",
    "                \"Job Title\" : job_title_list, \n",
    "                \"Company Name\" : company_list, \n",
    "                \"Company Location\" : location_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_jobmarket_df = pd.DataFrame(US_jobmarket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_jobmarket_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_jobmarket_df.to_csv(\"../Clean Data/US-JobMarket.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
